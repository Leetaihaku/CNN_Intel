2개층 컨브넷 - 중간 피처
2개층 전결합 - 중간 피처
= 현재 최적 96%
learning_rate = 1e-3
batch_size = 128
epochs = 3000
INPUT_SIZE = 32
KERNEL_SIZE = 5
CONV_IN = 3
CONV_MID1 = 22
CONV_OUT = 28
FC_IN = 5
DROP_PROB = 0.5

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    # 합성곱층(Output_size = ((input_size - kernel_size + 2 * padding_size)/stride_size + 1)
    self.conv1 = nn.Conv2d(in_channels=CONV_IN, out_channels=CONV_MID1, kernel_size=KERNEL_SIZE).cuda(device=DEVICE)
    self.conv2 = nn.Conv2d(in_channels=CONV_MID1, out_channels=CONV_OUT, kernel_size=KERNEL_SIZE).cuda(device=DEVICE)

    # 전결합층
    self.fc1 = nn.Linear(in_features=CONV_OUT*FC_IN*FC_IN, out_features=12).cuda()
    self.fc2 = nn.Linear(in_features=12, out_features=6).cuda()

  def forward(self, x):
    # 풀링층(Max_pooling = n size = n으로 나눈 값)
    x = F.max_pool2d(F.relu(self.conv1(x)).cuda(device=DEVICE), 2)
    x = F.max_pool2d(F.relu(self.conv2(x)).cuda(device=DEVICE), 2)
    x = x.view(-1, CONV_OUT*FC_IN*FC_IN)
    x = F.relu(self.fc1(x)).cuda(device=DEVICE)
    x = F.dropout(x, p=DROP_PROB)
    x = F.log_softmax(self.fc2(x), dim=1).cuda(device=DEVICE)
    return x